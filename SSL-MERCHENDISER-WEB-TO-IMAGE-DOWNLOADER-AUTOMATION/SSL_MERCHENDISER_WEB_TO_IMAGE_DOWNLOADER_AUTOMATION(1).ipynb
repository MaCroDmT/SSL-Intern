{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Install Chrome and the required Python libraries\n",
        "!apt-get update\n",
        "!wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -f -y\n",
        "!pip install selenium webdriver-manager\n",
        "\n",
        "# Part 2: Import libraries and set up Selenium\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "import shutil\n",
        "import re # We need this for regular expressions\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from google.colab import files\n",
        "\n",
        "# Set up headless options for the browser and add a custom User-Agent\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
        "chrome_options.binary_location = '/usr/bin/google-chrome'\n",
        "\n",
        "# Use webdriver-manager to handle ChromeDriver and create the browser instance\n",
        "service = ChromeService(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Part 3: The core scraping function - A more robust version\n",
        "def download_images_from_any_website(page_url, folder, update_progress, total_count_callback):\n",
        "    driver.get(page_url)\n",
        "    time.sleep(5)  # Give the page more time to load all content\n",
        "\n",
        "    # Scroll the entire page to the bottom to trigger lazy loading\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    # Find image URLs from various elements\n",
        "    images = driver.find_elements(By.XPATH, \"//img | //picture | //div[contains(@style, 'background-image')]\")\n",
        "    img_urls = set()\n",
        "    for img in images:\n",
        "        # Check standard image attributes\n",
        "        src = img.get_attribute(\"src\")\n",
        "        if src:\n",
        "            img_urls.add(src)\n",
        "\n",
        "        # Check srcset (for responsive images)\n",
        "        srcset = img.get_attribute(\"srcset\")\n",
        "        if srcset:\n",
        "            parts = [p.strip() for p in srcset.split(',')]\n",
        "            for part in parts:\n",
        "                url_part = part.split()[0]\n",
        "                if url_part:\n",
        "                    img_urls.add(url_part)\n",
        "\n",
        "        # Check data-src and other data attributes\n",
        "        data_src = img.get_attribute(\"data-src\")\n",
        "        if data_src:\n",
        "            img_urls.add(data_src)\n",
        "\n",
        "        # Check for background images in style attributes using regex\n",
        "        style = img.get_attribute(\"style\")\n",
        "        if style and 'background-image' in style:\n",
        "            try:\n",
        "                url_match = re.search(r'url\\(\"?\\'?(.+?)\"?\\'?\\)', style)\n",
        "                if url_match:\n",
        "                    img_urls.add(url_match.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    img_urls = list(img_urls)\n",
        "    total = len(img_urls)\n",
        "    total_count_callback(total)\n",
        "\n",
        "    if total == 0:\n",
        "        return 0\n",
        "\n",
        "    count = 0\n",
        "    for i, url in enumerate(img_urls, start=1):\n",
        "        try:\n",
        "            abs_url = urljoin(page_url, url)\n",
        "            response = requests.get(abs_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            if response.status_code == 200:\n",
        "                ext = os.path.splitext(abs_url)[1].split('?')[0]\n",
        "                if ext.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:\n",
        "                    ext = '.jpg'\n",
        "                filename = os.path.join(folder, f\"image_{i}{ext}\")\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {e}\")\n",
        "        update_progress(i)\n",
        "    return count\n",
        "\n",
        "# Part 4: The part to call the function and handle results (without a GUI)\n",
        "url = input(\"Enter The Page URL: \")\n",
        "folder = \"Web_Extracted_images\"\n",
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "try:\n",
        "    downloaded = download_images_from_any_website(url, folder, lambda x: None, lambda x: None)\n",
        "    print(f\"Downloaded {downloaded} images to the '{folder}' folder.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "    # Always quit the driver to free up resources\n",
        "    driver.quit()\n",
        "    # Part 5: Zip and download the folder\n",
        "    print(\"Zipping images...\")\n",
        "    shutil.make_archive('Web_Extracted_images', 'zip', 'Web_Extracted_images')\n",
        "    print(\"Download will start shortly.\")\n",
        "    files.download('Web_Extracted_images.zip')\n",
        "\n",
        "    # Part 6: Delete the extracted images folder\n",
        "    print(\"Cleaning up local files...\")\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "        print(f\"The '{folder}' folder has been deleted.\")\n",
        "    else:\n",
        "        print(f\"The '{folder}' folder does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "WqAFtfh_d2l6",
        "outputId": "f8edaca2-9975-4da6-a481-7c1d277d5939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.138.128.112)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.138.128.112)] [\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Connected to cloud.r-project.org (108.138.128.112)] [Connected to r2u.stat.\r                                                                               \rHit:5 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connected to cloud.r-project.org (108.138.128.112)] [Waiting for headers] [\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "(Reading database ... 126695 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (140.0.7339.127-1) over (140.0.7339.127-1) ...\n",
            "Setting up google-chrome-stable (140.0.7339.127-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.35.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (3.4.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Enter The Page URL: https://easyfashion.com.bd/\n",
            "Downloaded 156 images to the 'Web_Extracted_images' folder.\n",
            "Zipping images...\n",
            "Download will start shortly.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6b63eafe-3a99-42f1-9bb7-e03d17843a98\", \"Web_Extracted_images.zip\", 20392281)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up local files...\n",
            "The 'Web_Extracted_images' folder has been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excel to Zipped Version**"
      ],
      "metadata": {
        "id": "cWT4YqJAHUUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Part 1: Install Chrome and required libraries\n",
        "# ==============================================================\n",
        "!apt-get update -qq\n",
        "!wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -f -y -qq\n",
        "!pip install selenium webdriver-manager pandas openpyxl\n",
        "\n",
        "# ==============================================================\n",
        "# Part 2: Imports & Setup\n",
        "# ==============================================================\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import shutil\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from google.colab import files\n",
        "\n",
        "# Headless Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                            \"Chrome/91.0.4472.124 Safari/537.36\")\n",
        "chrome_options.binary_location = '/usr/bin/google-chrome'\n",
        "\n",
        "# Chrome driver service\n",
        "service = ChromeService(ChromeDriverManager().install())\n",
        "\n",
        "# ==============================================================\n",
        "# Part 3: Scraping function\n",
        "# ==============================================================\n",
        "def download_images_from_any_website(page_url, folder, update_progress, total_count_callback):\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    driver.get(page_url)\n",
        "    time.sleep(5)  # allow page to load\n",
        "\n",
        "    # Scroll to load lazy images\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    # Collect image URLs\n",
        "    images = driver.find_elements(By.XPATH, \"//img | //picture | //div[contains(@style, 'background-image')]\")\n",
        "    img_urls = set()\n",
        "    for img in images:\n",
        "        src = img.get_attribute(\"src\")\n",
        "        if src and 'data:image' not in src:\n",
        "            img_urls.add(src)\n",
        "\n",
        "        srcset = img.get_attribute(\"srcset\")\n",
        "        if srcset:\n",
        "            for part in [p.strip() for p in srcset.split(',')]:\n",
        "                url_part = part.split()[0]\n",
        "                if url_part:\n",
        "                    img_urls.add(url_part)\n",
        "\n",
        "        data_src = img.get_attribute(\"data-src\")\n",
        "        if data_src:\n",
        "            img_urls.add(data_src)\n",
        "\n",
        "        style = img.get_attribute(\"style\")\n",
        "        if style and 'background-image' in style:\n",
        "            try:\n",
        "                url_match = re.search(r'url\\(\"?\\'?(.+?)\"?\\'?\\)', style)\n",
        "                if url_match:\n",
        "                    img_urls.add(url_match.group(1))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    img_urls = list(img_urls)\n",
        "    total = len(img_urls)\n",
        "    total_count_callback(total)\n",
        "\n",
        "    if total == 0:\n",
        "        driver.quit()\n",
        "        return 0\n",
        "\n",
        "    count = 0\n",
        "    for i, url in enumerate(img_urls, start=1):\n",
        "        try:\n",
        "            abs_url = urljoin(page_url, url)\n",
        "            response = requests.get(abs_url, timeout=10,\n",
        "                                    headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            if response.status_code == 200:\n",
        "                ext = os.path.splitext(abs_url)[1].split('?')[0]\n",
        "                if not ext or ext.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:\n",
        "                    ext = '.jpg'\n",
        "                filename = os.path.join(folder, f\"image_{i}{ext}\")\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to download {url}: {e}\")\n",
        "        update_progress(i)\n",
        "\n",
        "    driver.quit()\n",
        "    return count\n",
        "\n",
        "# ==============================================================\n",
        "# Part 4: Main Execution\n",
        "# ==============================================================\n",
        "print(\"üì§ Please upload your Excel file (URLs in first column)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"‚ùå No file uploaded. Exiting.\")\n",
        "else:\n",
        "    excel_file = next(iter(uploaded))\n",
        "    print(f\"‚úÖ File '{excel_file}' uploaded successfully.\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(excel_file, header=None)\n",
        "\n",
        "        # Group URLs by domain\n",
        "        domain_urls = {}\n",
        "        for index, row in df.iterrows():\n",
        "            url = str(row.iloc[0]).strip()\n",
        "            if not url or not url.startswith(('http://', 'https://')):\n",
        "                print(f\"‚ö†Ô∏è Skipping invalid URL in row {index+1}: {url}\")\n",
        "                continue\n",
        "            domain = urlparse(url).netloc\n",
        "            domain = re.sub(r'[^a-zA-Z0-9]', '_', domain)  # safe folder name\n",
        "            domain_urls.setdefault(domain, []).append(url)\n",
        "\n",
        "        # Process each domain separately\n",
        "        for domain, urls in domain_urls.items():\n",
        "            print(f\"\\nüåê Processing domain: {domain}\")\n",
        "            domain_folder = f\"Images_{domain}\"\n",
        "            os.makedirs(domain_folder, exist_ok=True)\n",
        "\n",
        "            # Download images from each URL of this domain\n",
        "            for i, url in enumerate(urls, start=1):\n",
        "                print(f\"  üîó ({i}/{len(urls)}) {url}\")\n",
        "                subfolder = os.path.join(domain_folder, f\"url_{i}\")\n",
        "                os.makedirs(subfolder, exist_ok=True)\n",
        "                downloaded = download_images_from_any_website(\n",
        "                    url, subfolder, lambda x: None, lambda x: None\n",
        "                )\n",
        "                print(f\"   ‚úÖ {downloaded} images saved to {subfolder}\")\n",
        "\n",
        "            # Zip this domain‚Äôs images\n",
        "            zip_name = f\"{domain}.zip\"\n",
        "            shutil.make_archive(domain, 'zip', domain_folder)\n",
        "            print(f\"üì¶ Created {zip_name}\")\n",
        "\n",
        "            # Download immediately\n",
        "            files.download(zip_name)\n",
        "            print(f\"üì• Downloading {zip_name}...\")\n",
        "\n",
        "            # Cleanup\n",
        "            shutil.rmtree(domain_folder)\n",
        "            os.remove(zip_name)\n",
        "\n",
        "        print(\"\\nüéâ All domains processed and downloaded separately!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n"
      ],
      "metadata": {
        "id": "iuVdquBEHUAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}